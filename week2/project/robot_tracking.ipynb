{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the video file\n",
    "path = \"robots.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for good feature detection (finds good corners for tracking)\n",
    "feature_params = dict(maxCorners=500, qualityLevel=0.1, minDistance=5, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade Optical Flow (for tracking features across frames)\n",
    "lk_params = dict(winSize=(31, 31), maxLevel=1, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "# Threshold for movement (in pixels). Points moving less than this will be ignored.\n",
    "movement_threshold = 2 \n",
    "\n",
    "# Threshold for the clustering algorithm (maximum distance between points to form a cluster)\n",
    "distance_threshold = 150  # Adjust this value based on your scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "# Read the first frame of the video\n",
    "ret, old_frame = cap.read()\n",
    "\n",
    "if ret:\n",
    "    # Convert the first frame to grayscale (necessary for feature detection)\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect features in the first frame using goodFeaturesToTrack (corners)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "    # Create a mask image for drawing the tracks (initially a blank image)\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # Loop over the video frames\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # Break the loop if the video is over\n",
    "\n",
    "        # Convert the current frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate optical flow using Lucas-Kanade method\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        # Select only the points that were successfully tracked (st == 1)\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        # Loop through all tracked points\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            # Get the x, y coordinates of the new and old points\n",
    "            a, b = new.ravel() # new point in the current frame\n",
    "            c, d = old.ravel() # old point in the previous frame\n",
    "\n",
    "            # Calculate Euclidean distance between old and new points\n",
    "            movement = np.sqrt((a - c) ** 2 + (b - d) ** 2)\n",
    "\n",
    "            # Only display points that moved more than the threshold\n",
    "            if movement > movement_threshold:\n",
    "                # Draw a line from the old point to the new point (track of the feature)\n",
    "                mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), (0, 255, 0), 2)\n",
    "                # Draw a circle at the new point to indicate the current position\n",
    "                frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 0, 255), -1)\n",
    "\n",
    "        # Overlay the tracks (lines) onto the current frame\n",
    "        img = cv2.add(frame, mask)\n",
    "\n",
    "        # Display the current frame with the tracked points using OpenCV\n",
    "        cv2.imshow('Robot Tracking', img)\n",
    "\n",
    "        # Update the previous frame to the current frame (for the next iteration)\n",
    "        old_gray = frame_gray.copy()\n",
    "\n",
    "        # Update the previous points to the current points (for the next iteration)\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        # Exit loop when 'q' is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Optical Flow with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "# Read the first frame of the video\n",
    "ret, old_frame = cap.read()\n",
    "if ret:\n",
    "    # Convert the first frame to grayscale (necessary for feature detection)\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect features in the first frame using goodFeaturesToTrack (corners)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "    # Create a mask image for drawing the tracks (initially a blank image)\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # Loop over all video frames\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit the loop if no more frames are available\n",
    "\n",
    "        # Convert the current frame to grayscale (necessary for optical flow calculation)\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate optical flow to track the detected features in the new frame\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        # Select only the points that were successfully tracked (st == 1)\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        # Store moving points (after thresholding) to calculate the bounding box\n",
    "        moving_points = []\n",
    "\n",
    "        # Loop through all tracked points\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            # Get the x, y coordinates of the new and old points\n",
    "            a, b = new.ravel()  # new point in the current frame\n",
    "            c, d = old.ravel()  # old point from the previous frame\n",
    "\n",
    "            # Calculate the Euclidean distance between old and new points (movement)\n",
    "            movement = np.sqrt((a - c) ** 2 + (b - d) ** 2)\n",
    "\n",
    "            # Only draw the points that have moved more than the threshold\n",
    "            if movement > movement_threshold:\n",
    "                # Add the moving point to the list for clustering\n",
    "                moving_points.append([a, b])\n",
    "\n",
    "                # Draw a line from the old point to the new point (track of the feature)\n",
    "                mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), (0, 255, 0), 2)\n",
    "                # Draw a circle at the new point to indicate the current position\n",
    "                frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 0, 255), -1)\n",
    "\n",
    "        # Only perform clustering if there are moving points\n",
    "        if moving_points:\n",
    "            # Convert the moving points to a numpy array\n",
    "            moving_points = np.array(moving_points)\n",
    "\n",
    "            # Use DBSCAN to cluster the moving points based on their proximity\n",
    "            clustering = DBSCAN(eps=distance_threshold, min_samples=2).fit(moving_points)\n",
    "\n",
    "            # Get unique cluster labels\n",
    "            labels = clustering.labels_\n",
    "\n",
    "            # For each unique cluster, draw a bounding box\n",
    "            for cluster_label in np.unique(labels):\n",
    "                if cluster_label == -1:\n",
    "                    # Noise points are labeled as -1 (ignore these)\n",
    "                    continue\n",
    "\n",
    "                # Get the points belonging to the current cluster\n",
    "                cluster_points = moving_points[labels == cluster_label]\n",
    "\n",
    "                # Get the minimum and maximum x and y coordinates (bounding box)\n",
    "                x_min, y_min = np.min(cluster_points, axis=0)\n",
    "                x_max, y_max = np.max(cluster_points, axis=0)\n",
    "\n",
    "                # Draw the bounding box around the clustered points\n",
    "                frame = cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
    "\n",
    "        # Overlay the tracks (lines) onto the current frame\n",
    "        img = cv2.add(frame, mask)\n",
    "\n",
    "        # Display the current frame with the tracked points and bounding boxes using OpenCV\n",
    "        cv2.imshow('Robot Tracking', img)\n",
    "\n",
    "        # Update the previous frame to the current frame (for the next iteration)\n",
    "        old_gray = frame_gray.copy()\n",
    "\n",
    "        # Update the previous points to the current points (for the next iteration)\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        # Exit the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "# Read the first frame\n",
    "ret, old_frame = cap.read()\n",
    "\n",
    "if ret:\n",
    "    # Convert the first frame to grayscale\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Loop over the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the current frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate dense optical flow using the Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(old_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.5, 0)\n",
    "\n",
    "        # Compute the magnitude and angle of the 2D flow vectors\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Create a mask for visualization (Hue = angle, Value = magnitude)\n",
    "        hsv_mask = np.zeros_like(old_frame)\n",
    "        hsv_mask[..., 1] = 255  # Saturation to maximum\n",
    "        hsv_mask[..., 0] = angle * 180 / np.pi / 2  # Hue from the angle\n",
    "        hsv_mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)  # Value from magnitude\n",
    "\n",
    "        # Convert HSV to BGR for display\n",
    "        rgb_flow = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Blend the original frame with the optical flow visualization\n",
    "        overlaid_frame = cv2.addWeighted(frame, 0.7, rgb_flow, 0.3, 0)\n",
    "\n",
    "        # Display the overlaid frame\n",
    "        cv2.imshow('Dense Optical Flow Overlay', overlaid_frame)\n",
    "\n",
    "        # Update the previous frame\n",
    "        old_gray = frame_gray.copy()\n",
    "\n",
    "        # Exit loop when 'q' is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
