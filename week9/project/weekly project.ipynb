{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly project\n",
    "\n",
    "Today you are going to implement the last parts of the algorithm you started on monday. For reference you can see it below.\n",
    "\n",
    "![title](algorithm_3.png)\n",
    "\n",
    "It is a good idea to follow and track the steps in the algorithm in the below implementation. Only take one step at a time.\n",
    "\n",
    "Once you have the algorithm up and running you can try with a larger dataset to see if your algorithm is able to maintain good accurracy over a longer distance. The larger dataset can be found here:\n",
    "[Left images](https://dtudk-my.sharepoint.com/:u:/g/personal/evanb_dtu_dk/EQu8kmGBDDROtGJ7IkZB2tQBJrxmgY9t8LVM_JuEi83TYw)\n",
    "[Right images](https://dtudk-my.sharepoint.com/:u:/g/personal/evanb_dtu_dk/EcKI_zrXTvpMulizidCZm4oBLJcQ_LTV9Zs6oQFF74JTRQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "import glob\n",
    "\n",
    "def getK():\n",
    "    return np.array([[7.188560e+02, 0.000000e+00, 6.071928e+02],\n",
    "                     [0, 7.188560e+02, 1.852157e+02],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "def getTruePose():\n",
    "    file = '00.txt'\n",
    "    return np.genfromtxt(file, delimiter=' ', dtype=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>tips:</summary>\n",
    "\n",
    "- [Feature Matching](https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html)\n",
    "- To get the keypoint of a certain match do: ```kp1[match.queryIdx].pt``` and ```kp2[match.trainIdx].pt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and match keypoints using SIFT\n",
    "def extract_keypoints_sift(img1, img2, K, baseline):\n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect and compute descriptors for both images\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)  # Keypoints and descriptors for first image\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)  # Keypoints and descriptors for second image\n",
    "\n",
    "    # Create BFMatcher object with cross-check enabled for better matches\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)  # Match descriptors between images\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches by distance (quality)\n",
    "\n",
    "    # Extract matched points from keypoints\n",
    "    match_points1 = np.array([kp1[m.queryIdx].pt for m in matches], dtype='float32')\n",
    "    match_points2 = np.array([kp2[m.trainIdx].pt for m in matches], dtype='float32')\n",
    "\n",
    "    # Triangulate points to obtain 3D coordinates\n",
    "    M_left = K.dot(np.hstack((np.eye(3), np.zeros((3, 1)))))  # Projection matrix for left image\n",
    "    M_right = K.dot(np.hstack((np.eye(3), np.array([[-baseline, 0, 0]]).T)))  # Projection matrix for right image\n",
    "\n",
    "    # Prepare points for triangulation\n",
    "    p1_flip = np.vstack((match_points1.T, np.ones((1, match_points1.shape[0]))))\n",
    "    p2_flip = np.vstack((match_points2.T, np.ones((1, match_points2.shape[0]))))\n",
    "\n",
    "    # Perform triangulation\n",
    "    P = cv2.triangulatePoints(M_left, M_right, p1_flip[:2], p2_flip[:2])\n",
    "    P /= P[3]  # Normalize homogeneous coordinates\n",
    "\n",
    "    land_points = P[:3].T  # Convert to (N, 3) array of 3D points\n",
    "\n",
    "    return land_points, match_points1  # Return 3D points and 2D points from the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track features from one image to the next\n",
    "def featureTracking(prev_img, next_img, prev_points, world_points):\n",
    "    # Parameters for optical flow\n",
    "    params = dict(winSize=(21, 21),\n",
    "                  maxLevel=3,\n",
    "                  criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "    # Track points using optical flow\n",
    "    next_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_img, next_img, prev_points, None, **params)\n",
    "    status = status.flatten()  # Flatten status array for easier indexing\n",
    "\n",
    "    # Filter points that were successfully tracked\n",
    "    good_prev_points = prev_points[status == 1]  # Points in the previous image that were tracked\n",
    "    good_next_points = next_points[status == 1]  # Corresponding points in the next image\n",
    "    good_world_points = world_points[status == 1]  # Corresponding 3D world points\n",
    "\n",
    "    return good_world_points, good_prev_points, good_next_points  # Return filtered points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play the image sequence and estimate camera pose\n",
    "def playImageSequence(l_img_paths, r_img_paths, K, baseline):\n",
    "    baseline = 0.54\n",
    "    reference_img = cv2.imread(l_img_paths[0], 0)  # Load the initial reference image\n",
    "    truePose = getTruePose()  # Load ground truth poses\n",
    "    traj = np.zeros((600, 600, 3), dtype=np.uint8)  # Initialize an image for trajectory visualization\n",
    "    maxError = 0  # Track maximum error\n",
    "\n",
    "    for i in range(1, len(l_img_paths)):\n",
    "        curImage = cv2.imread(l_img_paths[i], 0)  # Load current left image\n",
    "        curImage_R = cv2.imread(r_img_paths[i], 0)  # Load current right image\n",
    "\n",
    "        # Step 1.2 and 1.3: Extract 3D and 2D correspondences\n",
    "        if i == 1:\n",
    "            # Initial feature extraction for the first pair of images\n",
    "            landmark_3D, reference_2D = extract_keypoints_sift(reference_img, curImage_R, K, baseline)\n",
    "        else:\n",
    "            # Track features between the previous and current images\n",
    "            landmark_3D, reference_2D, tracked_2Dpoints = featureTracking(reference_img, curImage, reference_2D, landmark_3D)\n",
    "\n",
    "        # Step 2.3: Estimate pose using PnP (Perspective-n-Point)\n",
    "        success, rvec, tvec, inliers = cv2.solvePnPRansac(landmark_3D, tracked_2Dpoints, K, None)\n",
    "\n",
    "        if success:\n",
    "            # Convert rotation vector to rotation matrix\n",
    "            rot, _ = cv2.Rodrigues(rvec)\n",
    "            tvec = -rot.T.dot(tvec)  # Convert camera pose to world coordinates\n",
    "            inv_transform = np.hstack((rot.T, tvec))  # Create transformation matrix\n",
    "            reference_img = curImage  # Update reference image for the next iteration\n",
    "\n",
    "            # Step 2.4: Triangulate new feature points\n",
    "            landmark_3D_new, reference_2D_new = extract_keypoints_sift(reference_img, curImage_R, K, baseline)\n",
    "            reference_2D = reference_2D_new.astype('float32')  # Update 2D reference points\n",
    "            # Transform new 3D points to world coordinates\n",
    "            landmark_3D = inv_transform.dot(np.vstack((landmark_3D_new.T, np.ones((1, landmark_3D_new.shape[0]))))).T\n",
    "\n",
    "            # Visualize the estimated pose and trajectory\n",
    "            draw_x, draw_y = int(tvec[0]) + 300, 600 - (int(tvec[2]) + 100)  # Convert coordinates for visualization\n",
    "            true_x, true_y = int(truePose[i][3]) + 300, 600 - (int(truePose[i][11]) + 100)  # Ground truth coordinates\n",
    "\n",
    "            # Calculate current error\n",
    "            curError = np.sqrt((tvec[0] - truePose[i][3]) ** 2 + (tvec[1] - truePose[i][7]) ** 2 + (tvec[2] - truePose[i][11]) ** 2)\n",
    "            maxError = max(maxError, curError)  # Update maximum error\n",
    "\n",
    "            # Draw trajectory on the visualization image\n",
    "            cv2.circle(traj, (draw_x, draw_y), 1, (0, 0, 255), 2)  # Draw estimated position\n",
    "            cv2.circle(traj, (true_x, true_y), 1, (255, 0, 0), 2)  # Draw ground truth position\n",
    "            text = f\"Coordinates: x ={tvec[0]:.2f}m y = {tvec[1]:.2f}m z = {tvec[2]:.2f}m\"  # Display coordinates\n",
    "            cv2.putText(traj, text, (10, 50), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, 8)  # Overlay text\n",
    "            vis = np.hstack((traj, np.dstack((curImage, curImage, curImage))))  # Combine trajectory and current image\n",
    "            cv2.imshow(\"Trajectory\", vis)  # Show visualization\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # Break loop if 'Esc' is pressed\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tracked_2Dpoints' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m right_img_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright/*.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m K \u001b[38;5;241m=\u001b[39m getK()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mplayImageSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_img_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_img_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m, in \u001b[0;36mplayImageSequence\u001b[1;34m(l_img_paths, r_img_paths, K, baseline)\u001b[0m\n\u001b[0;32m     19\u001b[0m     landmark_3D, reference_2D, tracked_2Dpoints \u001b[38;5;241m=\u001b[39m featureTracking(reference_img, curImage, reference_2D, landmark_3D)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Step 2.3: Estimate pose using PnP (Perspective-n-Point)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m success, rvec, tvec, inliers \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msolvePnPRansac(landmark_3D, \u001b[43mtracked_2Dpoints\u001b[49m, K, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Convert rotation vector to rotation matrix\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     rot, _ \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mRodrigues(rvec)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'tracked_2Dpoints' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Load image paths\n",
    "left_img_paths = sorted(glob.glob('left/*.png'))\n",
    "right_img_paths = sorted(glob.glob('right/*.png'))\n",
    "\n",
    "K = getK()\n",
    "playImageSequence(left_img_paths, right_img_paths, K, baseline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "The current implementation only uses features computed at the current timestep. However, as we process more images we potentially have a lot of features from previous timesteps that are still valid. The challenge is to expand the `extract_keypoints_surf(..., refPoints)` function by giving it old reference points. You should then combine your freshly computed features with the old features and remove all duplicates. This requires you to keep track of old features and 3D points.\n",
    "\n",
    "Hint 1: The following function `removeDuplicate` can be used for removing duplicates.\n",
    "\n",
    "Hint 2: you are not interested in points that are behind you, so remember to remove points that are negative in the direction you move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicate(queryPoints, refPoints, radius=5):\n",
    "    # remove duplicate points from new query points,\n",
    "    for i in range(len(queryPoints)):\n",
    "        query = queryPoints[i]\n",
    "        xliml, xlimh = query[0] - radius, query[0] + radius\n",
    "        yliml, ylimh = query[1] - radius, query[1] + radius\n",
    "        inside_x_lim_mask = (refPoints[:, 0] > xliml) & (refPoints[:, 0] < xlimh)\n",
    "        curr_kps_in_x_lim = refPoints[inside_x_lim_mask]\n",
    "\n",
    "        if curr_kps_in_x_lim.shape[0] != 0:\n",
    "            inside_y_lim_mask = (curr_kps_in_x_lim[:, 1] > yliml) & (curr_kps_in_x_lim[:, 1] < ylimh)\n",
    "            curr_kps_in_x_lim_and_y_lim = curr_kps_in_x_lim[inside_y_lim_mask, :]\n",
    "            if curr_kps_in_x_lim_and_y_lim.shape[0] != 0:\n",
    "                queryPoints[i] = np.array([0, 0])\n",
    "    return (queryPoints[:, 0] != 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
